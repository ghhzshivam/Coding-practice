{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17e3e9e52ee4e55892ad7fbe08e2aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7cfe5f245dc4bf9b39779ff0fcc55db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 18:00:31.295638: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2025-08-03 18:00:31.300929: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394325000 Hz\n",
      "2025-08-03 18:00:31.301644: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x60fbe0c75750 executing computations on platform Host. Devices:\n",
      "2025-08-03 18:00:31.301685: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2025-08-03 18:00:31.335032: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7e23335e3f10>,\n",
       " <keras.layers.core.Dense at 0x7e23312d58d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7e23c09d1dd0>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7e23c1625190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23ba429b10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23ba4298d0>,\n",
       " <keras.layers.core.Activation at 0x7e23bace9750>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7e23ba2d5950>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23ba23b3d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23ba219550>,\n",
       " <keras.layers.core.Activation at 0x7e23bac7bc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23ba1de450>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23ba135190>,\n",
       " <keras.layers.core.Activation at 0x7e23ba135350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b8437150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b8338bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b8357710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b82b11d0>,\n",
       " <keras.layers.merge.Add at 0x7e23b827fc90>,\n",
       " <keras.layers.core.Activation at 0x7e23b8240d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b81f0690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b81cea90>,\n",
       " <keras.layers.core.Activation at 0x7e23b81bad10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b80d0310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b80cb150>,\n",
       " <keras.layers.core.Activation at 0x7e23b80cb310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b07d15d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b0719e90>,\n",
       " <keras.layers.merge.Add at 0x7e23b06d6890>,\n",
       " <keras.layers.core.Activation at 0x7e23b06ebb10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b069e8d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b064efd0>,\n",
       " <keras.layers.core.Activation at 0x7e23b064eb10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b05e6e90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b0560150>,\n",
       " <keras.layers.core.Activation at 0x7e23b0560210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b047e790>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b0476990>,\n",
       " <keras.layers.merge.Add at 0x7e23b04120d0>,\n",
       " <keras.layers.core.Activation at 0x7e23b039aa90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b03211d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b030f0d0>,\n",
       " <keras.layers.core.Activation at 0x7e23b02fa690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b0292f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b01dab50>,\n",
       " <keras.layers.core.Activation at 0x7e23b02098d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b0182110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23b0084c90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b0121810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23b00b7e50>,\n",
       " <keras.layers.merge.Add at 0x7e238c740410>,\n",
       " <keras.layers.core.Activation at 0x7e238c756fd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c6a1f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c6d3790>,\n",
       " <keras.layers.core.Activation at 0x7e238c6d3e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c674f50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c57f190>,\n",
       " <keras.layers.core.Activation at 0x7e238c5d8690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c56fe50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c4d9b10>,\n",
       " <keras.layers.merge.Add at 0x7e238c4d9f10>,\n",
       " <keras.layers.core.Activation at 0x7e238c4051d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c426990>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c3ea4d0>,\n",
       " <keras.layers.core.Activation at 0x7e238c37dfd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c320c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c23a3d0>,\n",
       " <keras.layers.core.Activation at 0x7e238c282f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c21a850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c163a50>,\n",
       " <keras.layers.merge.Add at 0x7e238c1995d0>,\n",
       " <keras.layers.core.Activation at 0x7e238c132350>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e238c043050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e238c0a94d0>,\n",
       " <keras.layers.core.Activation at 0x7e238c0a9f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e237478ed90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2374727390>,\n",
       " <keras.layers.core.Activation at 0x7e237476e110>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23746f1610>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2374672f10>,\n",
       " <keras.layers.merge.Add at 0x7e2374672dd0>,\n",
       " <keras.layers.core.Activation at 0x7e237459e190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e237459e890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2374502a10>,\n",
       " <keras.layers.core.Activation at 0x7e2374517d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2374439c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23743bfe50>,\n",
       " <keras.layers.core.Activation at 0x7e2374417510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23743b1d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e237424c4d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e237431df50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23741ffe50>,\n",
       " <keras.layers.merge.Add at 0x7e23741ffc50>,\n",
       " <keras.layers.core.Activation at 0x7e2374151ad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2374105710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23740ccf50>,\n",
       " <keras.layers.core.Activation at 0x7e23740cc550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350fbf350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350fa1d90>,\n",
       " <keras.layers.core.Activation at 0x7e2350fa1a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350ebdd10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350eb4490>,\n",
       " <keras.layers.merge.Add at 0x7e2350eb4750>,\n",
       " <keras.layers.core.Activation at 0x7e2350dd3890>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350dd3b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350d47b10>,\n",
       " <keras.layers.core.Activation at 0x7e2350d47e50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350ce6850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350c2f790>,\n",
       " <keras.layers.core.Activation at 0x7e2350c66150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350b83210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350b61b10>,\n",
       " <keras.layers.merge.Add at 0x7e2350b61450>,\n",
       " <keras.layers.core.Activation at 0x7e2350a82cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350ce6d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350a79790>,\n",
       " <keras.layers.core.Activation at 0x7e2350a79f10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23509b2fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23508bbfd0>,\n",
       " <keras.layers.core.Activation at 0x7e2350913590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23508aae90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350817f50>,\n",
       " <keras.layers.merge.Add at 0x7e2350817ed0>,\n",
       " <keras.layers.core.Activation at 0x7e2350742290>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23509b2510>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2350728990>,\n",
       " <keras.layers.core.Activation at 0x7e23506ba850>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350658a90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23505d3ed0>,\n",
       " <keras.layers.core.Activation at 0x7e23505f4050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350570f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e235048c450>,\n",
       " <keras.layers.merge.Add at 0x7e23504d7ad0>,\n",
       " <keras.layers.core.Activation at 0x7e235046ded0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2350423210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23503edf10>,\n",
       " <keras.layers.core.Activation at 0x7e23503ed250>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e235032aa10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e235027d310>,\n",
       " <keras.layers.core.Activation at 0x7e235027d450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e235021a950>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23501965d0>,\n",
       " <keras.layers.merge.Add at 0x7e23501b1410>,\n",
       " <keras.layers.core.Activation at 0x7e2350133c90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e23500d2690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e235009ab50>,\n",
       " <keras.layers.core.Activation at 0x7e235009a6d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333f91210>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333f73bd0>,\n",
       " <keras.layers.core.Activation at 0x7e2333f73510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333ea8a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333da39d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333e07310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333d61090>,\n",
       " <keras.layers.merge.Add at 0x7e2333ca5490>,\n",
       " <keras.layers.core.Activation at 0x7e2333cbbe90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333bda750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333c38b10>,\n",
       " <keras.layers.core.Activation at 0x7e2333c38f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333b560d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333b3ef90>,\n",
       " <keras.layers.core.Activation at 0x7e2333b3ee10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333a6d690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23339cd890>,\n",
       " <keras.layers.merge.Add at 0x7e23339cd8d0>,\n",
       " <keras.layers.core.Activation at 0x7e233396bc10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e233390f710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e23338e5050>,\n",
       " <keras.layers.core.Activation at 0x7e23338e5990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e233381ead0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333780590>,\n",
       " <keras.layers.core.Activation at 0x7e2333780610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7e2333719710>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7e2333682f10>,\n",
       " <keras.layers.merge.Add at 0x7e2333682950>,\n",
       " <keras.layers.core.Activation at 0x7e2333631710>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7e233396b490>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7e23ba2828d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n",
      "  3/101 [..............................] - ETA: 1:44:19 - loss: 0.5243 - acc: 0.7433"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
