{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f00d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-20 01:27:20--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-04-20 01:27:21 (11.4 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8c220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "with open(r\"input.txt\", 'r+') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378771ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "char = sorted(set(text))\n",
    "char = \"\".join(char)\n",
    "\n",
    "vocab_size = len(char)\n",
    "print(char)\n",
    "print(len(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07355d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 39 a\n",
      "[46, 43, 50, 50, 53]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "# Building a simple tokenizer based on characters.\n",
    "\n",
    "stoi = { ch:i for i, ch in enumerate(char)}\n",
    "itos = { i:ch for i, ch in enumerate(char)}\n",
    "\n",
    "print('a', stoi['a'], itos[stoi['a']])\n",
    "\n",
    "# Encoding and decoding functions\n",
    "encode = lambda s: [stoi[ch] for ch in s] # Takes a string and output integers\n",
    "decode = lambda s: ''.join([itos[i] for i in s]) # Takes a list of integers as input and outpu a string\n",
    "\n",
    "print(encode('hello'))\n",
    "print(decode(encode('hello')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "285550ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) <built-in method size of Tensor object at 0x7d8f79bd6840>\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.size)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa518203",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88118fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb952294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18]) tensor(47)\n",
      "tensor([18, 47]) tensor(56)\n",
      "tensor([18, 47, 56]) tensor(57)\n",
      "tensor([18, 47, 56, 57]) tensor(58)\n",
      "tensor([18, 47, 56, 57, 58]) tensor(1)\n",
      "tensor([18, 47, 56, 57, 58,  1]) tensor(15)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15]) tensor(47)\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47]) tensor(58)\n"
     ]
    }
   ],
   "source": [
    "# Eg\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for i in range(block_size):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "\n",
    "    print(context, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbefb47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    \n",
    "    ix = torch.randint(0, len(data) - block_size, (batch_size,))            # Generating random posoitions and getting the data\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])     # Stacking in rows\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print(\"inputs: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"Targets: \")\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c88681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24]) tensor(43)\n",
      "tensor([24, 43]) tensor(58)\n",
      "tensor([24, 43, 58]) tensor(5)\n",
      "tensor([24, 43, 58,  5]) tensor(57)\n",
      "tensor([24, 43, 58,  5, 57]) tensor(1)\n",
      "tensor([24, 43, 58,  5, 57,  1]) tensor(46)\n",
      "tensor([24, 43, 58,  5, 57,  1, 46]) tensor(43)\n",
      "tensor([24, 43, 58,  5, 57,  1, 46, 43]) tensor(39)\n",
      "tensor([44]) tensor(53)\n",
      "tensor([44, 53]) tensor(56)\n",
      "tensor([44, 53, 56]) tensor(1)\n",
      "tensor([44, 53, 56,  1]) tensor(58)\n",
      "tensor([44, 53, 56,  1, 58]) tensor(46)\n",
      "tensor([44, 53, 56,  1, 58, 46]) tensor(39)\n",
      "tensor([44, 53, 56,  1, 58, 46, 39]) tensor(58)\n",
      "tensor([44, 53, 56,  1, 58, 46, 39, 58]) tensor(1)\n",
      "tensor([52]) tensor(58)\n",
      "tensor([52, 58]) tensor(1)\n",
      "tensor([52, 58,  1]) tensor(58)\n",
      "tensor([52, 58,  1, 58]) tensor(46)\n",
      "tensor([52, 58,  1, 58, 46]) tensor(39)\n",
      "tensor([52, 58,  1, 58, 46, 39]) tensor(58)\n",
      "tensor([52, 58,  1, 58, 46, 39, 58]) tensor(1)\n",
      "tensor([52, 58,  1, 58, 46, 39, 58,  1]) tensor(46)\n",
      "tensor([25]) tensor(17)\n",
      "tensor([25, 17]) tensor(27)\n",
      "tensor([25, 17, 27]) tensor(10)\n",
      "tensor([25, 17, 27, 10]) tensor(0)\n",
      "tensor([25, 17, 27, 10,  0]) tensor(21)\n",
      "tensor([25, 17, 27, 10,  0, 21]) tensor(1)\n",
      "tensor([25, 17, 27, 10,  0, 21,  1]) tensor(54)\n",
      "tensor([25, 17, 27, 10,  0, 21,  1, 54]) tensor(39)\n"
     ]
    }
   ],
   "source": [
    "# To see the batch\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(context, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d755ab10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
     ]
    }
   ],
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets = None):\n",
    "        logits = self.token_embedding_table(idx) #(B, T, C) (Batchm, time, channel)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            logits = logits.view(B*T, C) # (B*T, C)  # For getting loss we need to do this as we cannot feed b,t,c directly\n",
    "            targets = targets.view(B*T) # (B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :] # (B, C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples = 1) # (B, 1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108237f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18ba9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.572469472885132\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    #Sample batch\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Forward pass\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if steps % 10 == 0:\n",
    "    #     print(f\"Loss at step {steps}: {loss.item():.4f}\")\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5322f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iyoteng h hasbe pave pirance\n",
      "Rie hicomyonthar's\n",
      "Plinseard ith henoure wounonthioneir thondy, y helti\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df693e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0aa3581c",
   "metadata": {},
   "source": [
    "#### The Mathematical trick in sekf-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a008b602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72f70456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eca6f8",
   "metadata": {},
   "source": [
    "We are trying to couple in a way that info frmo prev go to future. We cannot have leakage so that info from, future available in past. We can average so that last one by one.\n",
    "So we may lose some data but for now it is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d12bb98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i] we mean so that that will be for last 2\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3fce387a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)  # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7871ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c1477a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completed video till: 01:02 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9149e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!  Decoder block\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dc7934f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.5103e-02, -2.3612e-01, -4.5171e-01,  4.7792e-01,  5.0296e-01,\n",
       "          -4.0769e-01, -1.2294e-01, -4.6356e-02,  3.1703e-01,  2.4524e-01,\n",
       "          -1.9162e-01,  5.3467e-01,  3.7165e-01,  2.1231e-01,  5.9100e-01,\n",
       "           1.8384e-01],\n",
       "         [ 4.9167e-01, -5.6461e-01, -5.9052e-01,  4.4887e-01,  7.7757e-01,\n",
       "          -7.6121e-01,  1.4188e-02,  1.6654e-01,  4.7274e-01,  2.1897e-01,\n",
       "          -5.3842e-01,  5.1860e-01,  4.4462e-01,  4.9806e-01,  7.5010e-01,\n",
       "           1.0307e-01],\n",
       "         [ 1.5293e-01,  1.3120e-02, -3.4630e-01,  2.0143e-01,  2.3533e-01,\n",
       "          -2.3894e-01,  1.7949e-02,  1.8566e-01, -9.8037e-02, -1.6844e-01,\n",
       "           5.3766e-02,  1.8995e-01,  5.0373e-02, -1.0214e-01, -4.0242e-02,\n",
       "           6.5572e-01],\n",
       "         [ 9.3663e-02,  3.5403e-01, -1.0914e-01, -9.4043e-02,  1.0841e-01,\n",
       "           2.5260e-01,  5.8558e-02, -6.2868e-02, -3.8557e-01, -7.3532e-02,\n",
       "           8.6651e-02, -8.5347e-02, -2.2574e-01, -1.9268e-01,  1.0780e-01,\n",
       "           6.0662e-01],\n",
       "         [ 7.9947e-03,  5.2671e-01,  1.1199e-01,  3.2813e-01,  2.3045e-01,\n",
       "           4.7927e-01,  2.3350e-01,  3.5107e-01, -2.8375e-01, -5.7586e-01,\n",
       "          -1.2519e-01,  7.9149e-02, -3.7526e-01, -1.1058e-01, -8.6856e-02,\n",
       "           3.6823e-01],\n",
       "         [ 2.6364e-01,  4.6007e-01,  3.0399e-01,  2.7267e-01,  1.5919e-01,\n",
       "           4.6152e-01,  3.0374e-01,  3.5349e-01, -4.2580e-01, -6.5305e-01,\n",
       "          -6.1246e-02, -1.7457e-01, -5.1340e-01,  5.8463e-02, -2.1527e-01,\n",
       "           6.8822e-01],\n",
       "         [ 1.8858e-01, -4.2830e-02, -2.5405e-01,  3.1645e-01,  9.0842e-02,\n",
       "           3.4490e-02, -2.0183e-01,  1.9068e-02, -4.9683e-02,  2.6372e-01,\n",
       "           2.9270e-01,  1.5392e-01,  1.2782e-01,  2.4085e-02,  8.5399e-02,\n",
       "           6.4211e-01],\n",
       "         [ 1.3013e-01, -3.2832e-02, -4.9645e-01,  2.8652e-01,  2.7042e-01,\n",
       "          -2.6357e-01, -7.3756e-02,  3.7857e-01,  7.4580e-02,  3.3827e-02,\n",
       "           1.4695e-02,  3.1937e-01,  2.9926e-01, -1.6530e-01, -3.8630e-02,\n",
       "           3.3748e-01]],\n",
       "\n",
       "        [[-1.3509e-01,  5.2079e-01,  4.5772e-01, -1.0386e-01, -4.6070e-01,\n",
       "           8.2143e-02, -3.6936e-02,  4.8745e-01,  5.3725e-02,  4.1700e-01,\n",
       "           3.7872e-01, -2.9301e-01,  5.2287e-01, -4.5193e-01,  1.8400e-01,\n",
       "          -5.4227e-01],\n",
       "         [ 4.6164e-02, -8.8216e-02,  3.6141e-01,  3.8638e-01,  4.5846e-01,\n",
       "           4.1429e-01, -3.1946e-01,  2.1412e-01,  3.2019e-01,  6.5214e-01,\n",
       "           1.0037e-01,  1.8674e-01,  5.8996e-01, -2.2666e-01, -1.1727e-01,\n",
       "          -1.6827e-01],\n",
       "         [-9.8031e-02,  4.4935e-01,  4.5277e-01, -1.2285e-01, -5.2487e-01,\n",
       "          -3.3399e-02, -1.2875e-01,  3.0734e-01, -1.4331e-02,  4.3599e-01,\n",
       "           2.8138e-01, -4.4214e-01,  4.9552e-01, -5.7658e-01,  2.1870e-01,\n",
       "          -5.7252e-01],\n",
       "         [-7.2463e-01,  3.8168e-01,  3.0158e-01, -1.5104e-01, -4.2773e-02,\n",
       "           2.1597e-01,  2.4977e-01,  2.5002e-01,  5.5421e-02,  3.0425e-02,\n",
       "          -3.7169e-01,  8.8684e-02,  5.0108e-01, -6.5775e-01,  1.0013e-01,\n",
       "          -2.9643e-01],\n",
       "         [ 1.4799e-01, -5.0172e-02,  5.3407e-01,  4.6679e-01,  2.8810e-01,\n",
       "           5.1234e-01, -2.2781e-01,  2.4689e-01,  2.2747e-01,  6.4787e-01,\n",
       "           6.5685e-02,  1.1113e-01,  4.7589e-01, -1.3726e-01, -1.9116e-02,\n",
       "          -1.5954e-01],\n",
       "         [-1.3920e-01,  1.4147e-01,  1.0953e-01, -3.0514e-01, -3.6496e-01,\n",
       "          -1.0889e-01, -1.1196e-01,  2.2308e-01,  2.8146e-01,  1.5394e-01,\n",
       "          -2.9628e-01, -5.6076e-01,  9.3148e-02, -1.0165e+00,  2.3237e-01,\n",
       "          -4.3836e-01],\n",
       "         [ 3.4840e-02,  1.5845e-01,  3.7821e-01,  1.4291e-01, -2.2733e-01,\n",
       "           2.0836e-01, -6.4442e-02,  2.1680e-01,  7.7749e-03,  4.3365e-01,\n",
       "           3.0704e-01, -9.0311e-02,  3.2026e-01, -4.1883e-01,  5.5491e-03,\n",
       "          -4.6036e-01],\n",
       "         [-5.3896e-01,  7.5555e-01,  3.3034e-01, -1.5849e-01, -2.6740e-01,\n",
       "           4.3495e-01,  3.7772e-01,  5.5794e-01, -1.8369e-01,  1.5938e-01,\n",
       "          -2.1042e-01,  5.5790e-02,  6.3184e-01, -6.4884e-01, -9.6084e-02,\n",
       "          -5.0751e-01]],\n",
       "\n",
       "        [[-2.9159e-02,  6.3748e-02,  1.2793e-01,  3.5261e-01, -3.0779e-01,\n",
       "          -3.1751e-01, -3.1972e-01,  2.0106e-01, -1.2531e-01,  2.9093e-01,\n",
       "           2.6410e-01,  3.1269e-01,  4.5849e-02,  1.3145e-01, -1.9723e-01,\n",
       "           5.6100e-02],\n",
       "         [-3.9130e-02,  3.1482e-01,  2.2664e-02,  1.9118e-02, -3.8114e-02,\n",
       "          -3.0549e-01, -2.3376e-01, -3.3880e-02,  9.9978e-02,  2.8291e-01,\n",
       "           1.4982e-02,  2.2472e-01, -1.5772e-01, -5.0688e-02,  1.3870e-01,\n",
       "           9.1078e-02],\n",
       "         [ 2.7041e-01,  2.4549e-01, -6.7637e-02,  5.0701e-02,  6.9261e-01,\n",
       "          -3.4878e-02, -4.0591e-01,  1.9950e-01, -1.1595e-01,  7.0213e-01,\n",
       "          -4.2482e-01,  1.6534e-01, -7.8323e-02,  3.1561e-01,  1.2386e-01,\n",
       "           4.3812e-02],\n",
       "         [-1.0703e+00, -3.2667e-01, -3.8559e-01,  1.4783e-01, -2.1840e-01,\n",
       "          -6.3213e-01,  5.8073e-02,  3.0096e-01,  2.4579e-01, -8.0825e-02,\n",
       "           4.7087e-01,  4.5685e-01, -2.3482e-01, -4.7984e-01,  1.0081e-01,\n",
       "           1.3407e-02],\n",
       "         [ 3.6790e-01,  6.9248e-01,  2.0775e-01,  1.5045e-01, -5.6374e-01,\n",
       "          -2.8797e-01,  2.8236e-02, -2.3397e-01, -8.5428e-02,  2.5549e-01,\n",
       "           1.6044e-01,  4.4575e-01, -1.4006e-01, -1.8100e-01, -2.8638e-03,\n",
       "           4.4905e-02],\n",
       "         [-3.6897e-01,  1.5715e-02, -3.0724e-01,  2.4265e-01,  1.0801e-02,\n",
       "          -2.5828e-01,  1.7035e-02,  1.4178e-01,  2.5009e-01,  1.4988e-01,\n",
       "           1.9546e-01,  4.1566e-01, -1.5199e-01, -1.4026e-01,  3.2418e-01,\n",
       "           1.6767e-02],\n",
       "         [-7.4233e-02,  1.2264e-02, -2.8135e-01,  3.0379e-01,  3.7928e-01,\n",
       "          -6.3295e-02, -1.4766e-01,  2.6890e-01,  1.0046e-01,  4.0451e-01,\n",
       "          -6.3321e-02,  3.5826e-01, -5.5159e-02,  1.5616e-01,  2.7065e-01,\n",
       "          -2.5203e-02],\n",
       "         [-2.4821e-01,  1.4845e-01, -3.5033e-01,  1.7102e-01,  1.6613e-01,\n",
       "          -2.0643e-01,  8.6633e-02,  8.8414e-02,  2.1188e-01,  2.5805e-01,\n",
       "           5.5145e-02,  4.2668e-01, -2.0443e-01, -1.7372e-01,  3.8899e-01,\n",
       "           5.1725e-02]],\n",
       "\n",
       "        [[-7.8777e-03,  1.3901e-01, -5.1703e-03, -4.4605e-01, -8.4715e-02,\n",
       "          -2.7588e-01, -2.0501e-01, -1.8734e-01, -2.7997e-01,  2.1082e-02,\n",
       "           1.2757e-01, -1.2263e-01,  1.3703e-01,  1.1513e-01, -6.5403e-01,\n",
       "           5.1961e-01],\n",
       "         [-1.4787e-01,  6.1818e-01,  5.4799e-01, -5.5381e-01, -2.5565e-01,\n",
       "          -2.1315e-02,  7.8645e-01, -3.4693e-01, -5.0124e-01, -5.5697e-02,\n",
       "          -9.4604e-02, -7.5704e-03, -1.4896e-02,  4.0720e-01, -2.3588e-01,\n",
       "           6.4970e-01],\n",
       "         [-4.4220e-02, -2.4991e-01, -6.7646e-02, -2.3050e-01, -2.2014e-04,\n",
       "          -1.4124e-01, -4.8434e-01, -3.8624e-01, -3.5337e-01, -1.9155e-01,\n",
       "           2.8412e-01,  3.4426e-01,  1.2248e-01, -1.0078e-01, -5.2063e-01,\n",
       "           1.0666e-01],\n",
       "         [-3.7465e-02, -1.8780e-02, -9.6524e-02, -3.0802e-01, -7.1619e-02,\n",
       "          -3.9649e-02, -3.1277e-01, -5.5705e-01, -6.1802e-01, -3.7121e-01,\n",
       "           2.3234e-01,  4.2150e-01,  2.4131e-01,  5.0146e-03, -5.6683e-01,\n",
       "           1.7352e-01],\n",
       "         [-7.0657e-01,  2.1864e-01,  1.2503e-01, -8.4264e-02,  3.9886e-02,\n",
       "          -6.6701e-02, -3.5455e-01,  1.3685e-01, -6.3320e-01, -2.3314e-01,\n",
       "           1.1689e-01, -3.8235e-01,  1.6702e-01,  1.8594e-02, -6.9968e-01,\n",
       "           4.3950e-01],\n",
       "         [ 2.0813e-01, -2.7916e-01, -4.7123e-01,  9.7088e-02, -1.7485e-01,\n",
       "          -2.8349e-01, -8.4478e-01, -6.3890e-01, -4.7767e-01, -2.2004e-01,\n",
       "           2.1086e-01,  2.0564e-01,  3.4436e-02,  2.6234e-02, -7.4842e-01,\n",
       "           4.6231e-01],\n",
       "         [-4.2531e-01,  2.6437e-01,  4.9929e-01, -4.5409e-01, -1.6551e-02,\n",
       "          -3.0874e-01, -1.2841e-01,  1.1757e-01,  1.0417e-01,  1.6075e-01,\n",
       "           1.0330e-01, -6.2461e-01,  3.8366e-01,  3.2544e-01, -5.5554e-01,\n",
       "           5.1832e-01],\n",
       "         [-3.5927e-01,  3.3935e-02, -2.9863e-02, -1.5019e-01, -6.0354e-03,\n",
       "          -6.5733e-02, -3.9659e-01, -6.0435e-02, -5.7551e-01, -2.9157e-01,\n",
       "           1.4899e-01, -7.5002e-02,  7.3228e-02, -4.7413e-02, -6.4394e-01,\n",
       "           2.8560e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d74821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!  encoder block used for sentiment classification\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "\n",
    "# If encoder block so what we do is don'y use trill to mask so we have all the matrics rather than triangular matrics as we need for whole and will just generate single output embeddings.\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35638232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.5103e-02, -2.3612e-01, -4.5171e-01,  4.7792e-01,  5.0296e-01,\n",
       "          -4.0769e-01, -1.2294e-01, -4.6356e-02,  3.1703e-01,  2.4524e-01,\n",
       "          -1.9162e-01,  5.3467e-01,  3.7165e-01,  2.1231e-01,  5.9100e-01,\n",
       "           1.8384e-01],\n",
       "         [ 4.9167e-01, -5.6461e-01, -5.9052e-01,  4.4887e-01,  7.7757e-01,\n",
       "          -7.6121e-01,  1.4188e-02,  1.6654e-01,  4.7274e-01,  2.1897e-01,\n",
       "          -5.3842e-01,  5.1860e-01,  4.4462e-01,  4.9806e-01,  7.5010e-01,\n",
       "           1.0307e-01],\n",
       "         [ 1.5293e-01,  1.3120e-02, -3.4630e-01,  2.0143e-01,  2.3533e-01,\n",
       "          -2.3894e-01,  1.7949e-02,  1.8566e-01, -9.8037e-02, -1.6844e-01,\n",
       "           5.3766e-02,  1.8995e-01,  5.0373e-02, -1.0214e-01, -4.0242e-02,\n",
       "           6.5572e-01],\n",
       "         [ 9.3663e-02,  3.5403e-01, -1.0914e-01, -9.4043e-02,  1.0841e-01,\n",
       "           2.5260e-01,  5.8558e-02, -6.2868e-02, -3.8557e-01, -7.3532e-02,\n",
       "           8.6651e-02, -8.5347e-02, -2.2574e-01, -1.9268e-01,  1.0780e-01,\n",
       "           6.0662e-01],\n",
       "         [ 7.9947e-03,  5.2671e-01,  1.1199e-01,  3.2813e-01,  2.3045e-01,\n",
       "           4.7927e-01,  2.3350e-01,  3.5107e-01, -2.8375e-01, -5.7586e-01,\n",
       "          -1.2519e-01,  7.9149e-02, -3.7526e-01, -1.1058e-01, -8.6856e-02,\n",
       "           3.6823e-01],\n",
       "         [ 2.6364e-01,  4.6007e-01,  3.0399e-01,  2.7267e-01,  1.5919e-01,\n",
       "           4.6152e-01,  3.0374e-01,  3.5349e-01, -4.2580e-01, -6.5305e-01,\n",
       "          -6.1246e-02, -1.7457e-01, -5.1340e-01,  5.8463e-02, -2.1527e-01,\n",
       "           6.8822e-01],\n",
       "         [ 1.8858e-01, -4.2830e-02, -2.5405e-01,  3.1645e-01,  9.0842e-02,\n",
       "           3.4490e-02, -2.0183e-01,  1.9068e-02, -4.9683e-02,  2.6372e-01,\n",
       "           2.9270e-01,  1.5392e-01,  1.2782e-01,  2.4085e-02,  8.5399e-02,\n",
       "           6.4211e-01],\n",
       "         [ 1.3013e-01, -3.2832e-02, -4.9645e-01,  2.8652e-01,  2.7042e-01,\n",
       "          -2.6357e-01, -7.3756e-02,  3.7857e-01,  7.4580e-02,  3.3827e-02,\n",
       "           1.4695e-02,  3.1937e-01,  2.9926e-01, -1.6530e-01, -3.8630e-02,\n",
       "           3.3748e-01]],\n",
       "\n",
       "        [[-1.3509e-01,  5.2079e-01,  4.5772e-01, -1.0386e-01, -4.6070e-01,\n",
       "           8.2143e-02, -3.6936e-02,  4.8745e-01,  5.3725e-02,  4.1700e-01,\n",
       "           3.7872e-01, -2.9301e-01,  5.2287e-01, -4.5193e-01,  1.8400e-01,\n",
       "          -5.4227e-01],\n",
       "         [ 4.6164e-02, -8.8216e-02,  3.6141e-01,  3.8638e-01,  4.5846e-01,\n",
       "           4.1429e-01, -3.1946e-01,  2.1412e-01,  3.2019e-01,  6.5214e-01,\n",
       "           1.0037e-01,  1.8674e-01,  5.8996e-01, -2.2666e-01, -1.1727e-01,\n",
       "          -1.6827e-01],\n",
       "         [-9.8031e-02,  4.4935e-01,  4.5277e-01, -1.2285e-01, -5.2487e-01,\n",
       "          -3.3399e-02, -1.2875e-01,  3.0734e-01, -1.4331e-02,  4.3599e-01,\n",
       "           2.8138e-01, -4.4214e-01,  4.9552e-01, -5.7658e-01,  2.1870e-01,\n",
       "          -5.7252e-01],\n",
       "         [-7.2463e-01,  3.8168e-01,  3.0158e-01, -1.5104e-01, -4.2773e-02,\n",
       "           2.1597e-01,  2.4977e-01,  2.5002e-01,  5.5421e-02,  3.0425e-02,\n",
       "          -3.7169e-01,  8.8684e-02,  5.0108e-01, -6.5775e-01,  1.0013e-01,\n",
       "          -2.9643e-01],\n",
       "         [ 1.4799e-01, -5.0172e-02,  5.3407e-01,  4.6679e-01,  2.8810e-01,\n",
       "           5.1234e-01, -2.2781e-01,  2.4689e-01,  2.2747e-01,  6.4787e-01,\n",
       "           6.5685e-02,  1.1113e-01,  4.7589e-01, -1.3726e-01, -1.9116e-02,\n",
       "          -1.5954e-01],\n",
       "         [-1.3920e-01,  1.4147e-01,  1.0953e-01, -3.0514e-01, -3.6496e-01,\n",
       "          -1.0889e-01, -1.1196e-01,  2.2308e-01,  2.8146e-01,  1.5394e-01,\n",
       "          -2.9628e-01, -5.6076e-01,  9.3148e-02, -1.0165e+00,  2.3237e-01,\n",
       "          -4.3836e-01],\n",
       "         [ 3.4840e-02,  1.5845e-01,  3.7821e-01,  1.4291e-01, -2.2733e-01,\n",
       "           2.0836e-01, -6.4442e-02,  2.1680e-01,  7.7749e-03,  4.3365e-01,\n",
       "           3.0704e-01, -9.0311e-02,  3.2026e-01, -4.1883e-01,  5.5491e-03,\n",
       "          -4.6036e-01],\n",
       "         [-5.3896e-01,  7.5555e-01,  3.3034e-01, -1.5849e-01, -2.6740e-01,\n",
       "           4.3495e-01,  3.7772e-01,  5.5794e-01, -1.8369e-01,  1.5938e-01,\n",
       "          -2.1042e-01,  5.5790e-02,  6.3184e-01, -6.4884e-01, -9.6084e-02,\n",
       "          -5.0751e-01]],\n",
       "\n",
       "        [[-2.9159e-02,  6.3748e-02,  1.2793e-01,  3.5261e-01, -3.0779e-01,\n",
       "          -3.1751e-01, -3.1972e-01,  2.0106e-01, -1.2531e-01,  2.9093e-01,\n",
       "           2.6410e-01,  3.1269e-01,  4.5849e-02,  1.3145e-01, -1.9723e-01,\n",
       "           5.6100e-02],\n",
       "         [-3.9130e-02,  3.1482e-01,  2.2664e-02,  1.9118e-02, -3.8114e-02,\n",
       "          -3.0549e-01, -2.3376e-01, -3.3880e-02,  9.9978e-02,  2.8291e-01,\n",
       "           1.4982e-02,  2.2472e-01, -1.5772e-01, -5.0688e-02,  1.3870e-01,\n",
       "           9.1078e-02],\n",
       "         [ 2.7041e-01,  2.4549e-01, -6.7637e-02,  5.0701e-02,  6.9261e-01,\n",
       "          -3.4878e-02, -4.0591e-01,  1.9950e-01, -1.1595e-01,  7.0213e-01,\n",
       "          -4.2482e-01,  1.6534e-01, -7.8323e-02,  3.1561e-01,  1.2386e-01,\n",
       "           4.3812e-02],\n",
       "         [-1.0703e+00, -3.2667e-01, -3.8559e-01,  1.4783e-01, -2.1840e-01,\n",
       "          -6.3213e-01,  5.8073e-02,  3.0096e-01,  2.4579e-01, -8.0825e-02,\n",
       "           4.7087e-01,  4.5685e-01, -2.3482e-01, -4.7984e-01,  1.0081e-01,\n",
       "           1.3407e-02],\n",
       "         [ 3.6790e-01,  6.9248e-01,  2.0775e-01,  1.5045e-01, -5.6374e-01,\n",
       "          -2.8797e-01,  2.8236e-02, -2.3397e-01, -8.5428e-02,  2.5549e-01,\n",
       "           1.6044e-01,  4.4575e-01, -1.4006e-01, -1.8100e-01, -2.8638e-03,\n",
       "           4.4905e-02],\n",
       "         [-3.6897e-01,  1.5715e-02, -3.0724e-01,  2.4265e-01,  1.0801e-02,\n",
       "          -2.5828e-01,  1.7035e-02,  1.4178e-01,  2.5009e-01,  1.4988e-01,\n",
       "           1.9546e-01,  4.1566e-01, -1.5199e-01, -1.4026e-01,  3.2418e-01,\n",
       "           1.6767e-02],\n",
       "         [-7.4233e-02,  1.2264e-02, -2.8135e-01,  3.0379e-01,  3.7928e-01,\n",
       "          -6.3295e-02, -1.4766e-01,  2.6890e-01,  1.0046e-01,  4.0451e-01,\n",
       "          -6.3321e-02,  3.5826e-01, -5.5159e-02,  1.5616e-01,  2.7065e-01,\n",
       "          -2.5203e-02],\n",
       "         [-2.4821e-01,  1.4845e-01, -3.5033e-01,  1.7102e-01,  1.6613e-01,\n",
       "          -2.0643e-01,  8.6633e-02,  8.8414e-02,  2.1188e-01,  2.5805e-01,\n",
       "           5.5145e-02,  4.2668e-01, -2.0443e-01, -1.7372e-01,  3.8899e-01,\n",
       "           5.1725e-02]],\n",
       "\n",
       "        [[-7.8777e-03,  1.3901e-01, -5.1703e-03, -4.4605e-01, -8.4715e-02,\n",
       "          -2.7588e-01, -2.0501e-01, -1.8734e-01, -2.7997e-01,  2.1082e-02,\n",
       "           1.2757e-01, -1.2263e-01,  1.3703e-01,  1.1513e-01, -6.5403e-01,\n",
       "           5.1961e-01],\n",
       "         [-1.4787e-01,  6.1818e-01,  5.4799e-01, -5.5381e-01, -2.5565e-01,\n",
       "          -2.1315e-02,  7.8645e-01, -3.4693e-01, -5.0124e-01, -5.5697e-02,\n",
       "          -9.4604e-02, -7.5704e-03, -1.4896e-02,  4.0720e-01, -2.3588e-01,\n",
       "           6.4970e-01],\n",
       "         [-4.4220e-02, -2.4991e-01, -6.7646e-02, -2.3050e-01, -2.2014e-04,\n",
       "          -1.4124e-01, -4.8434e-01, -3.8624e-01, -3.5337e-01, -1.9155e-01,\n",
       "           2.8412e-01,  3.4426e-01,  1.2248e-01, -1.0078e-01, -5.2063e-01,\n",
       "           1.0666e-01],\n",
       "         [-3.7465e-02, -1.8780e-02, -9.6524e-02, -3.0802e-01, -7.1619e-02,\n",
       "          -3.9649e-02, -3.1277e-01, -5.5705e-01, -6.1802e-01, -3.7121e-01,\n",
       "           2.3234e-01,  4.2150e-01,  2.4131e-01,  5.0146e-03, -5.6683e-01,\n",
       "           1.7352e-01],\n",
       "         [-7.0657e-01,  2.1864e-01,  1.2503e-01, -8.4264e-02,  3.9886e-02,\n",
       "          -6.6701e-02, -3.5455e-01,  1.3685e-01, -6.3320e-01, -2.3314e-01,\n",
       "           1.1689e-01, -3.8235e-01,  1.6702e-01,  1.8594e-02, -6.9968e-01,\n",
       "           4.3950e-01],\n",
       "         [ 2.0813e-01, -2.7916e-01, -4.7123e-01,  9.7088e-02, -1.7485e-01,\n",
       "          -2.8349e-01, -8.4478e-01, -6.3890e-01, -4.7767e-01, -2.2004e-01,\n",
       "           2.1086e-01,  2.0564e-01,  3.4436e-02,  2.6234e-02, -7.4842e-01,\n",
       "           4.6231e-01],\n",
       "         [-4.2531e-01,  2.6437e-01,  4.9929e-01, -4.5409e-01, -1.6551e-02,\n",
       "          -3.0874e-01, -1.2841e-01,  1.1757e-01,  1.0417e-01,  1.6075e-01,\n",
       "           1.0330e-01, -6.2461e-01,  3.8366e-01,  3.2544e-01, -5.5554e-01,\n",
       "           5.1832e-01],\n",
       "         [-3.5927e-01,  3.3935e-02, -2.9863e-02, -1.5019e-01, -6.0354e-03,\n",
       "          -6.5733e-02, -3.9659e-01, -6.0435e-02, -5.7551e-01, -2.9157e-01,\n",
       "           1.4899e-01, -7.5002e-02,  7.3228e-02, -4.7413e-02, -6.4394e-01,\n",
       "           2.8560e-01]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b070fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
